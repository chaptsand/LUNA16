import argparse
import os
import sys
import time
from torch.utils.data import Dataset, DataLoader
from dsets import LunaDataset
from tqdm import tqdm # å¼ºçƒˆå»ºè®®å®‰è£…: pip install tqdm

# 1. å®šä¹‰ä¸€ä¸ªä¸“é—¨ç”¨äºç¼“å­˜çš„ Dataset
class PrecacheDataset(Dataset):
    def __init__(self, luna_ds):
        self.luna_ds = luna_ds
        # è·å–æ‰€æœ‰å”¯ä¸€çš„ seriesuid (å»é‡)
        self.series_uids = sorted(self.luna_ds.seriesuid_to_path.keys())
        print(f"æ€»å…±æœ‰ {len(self.series_uids)} ä¸ªå”¯ä¸€çš„ CT æ‰«æéœ€è¦æ£€æŸ¥/å¤„ç†ã€‚")

    def __len__(self):
        return len(self.series_uids)

    def __getitem__(self, index):
        # è¿™é‡Œæˆ‘ä»¬åªå…³å¿ƒâ€œè§¦å‘ç¼“å­˜è®¡ç®—â€ï¼Œä¸éœ€è¦è¿”å›å…·ä½“çš„å¤§æ•°ç»„
        uid = self.series_uids[index]
        try:
            # è°ƒç”¨ dsets.py ä¸­çš„æ ¸å¿ƒé€»è¾‘
            # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œå®ƒä¼šç›´æ¥è¿”å›ï¼›å¦‚æœä¸å­˜åœ¨ï¼Œå®ƒä¼šè®¡ç®—å¹¶ä¿å­˜
            self.luna_ds.get_resampled_ct(uid)
            return True # è¿”å›ä¸€ä¸ªç®€å•çš„æ ‡å¿—
        except Exception as e:
            print(f"Error processing {uid}: {e}")
            return False

def main():
    # --- é…ç½®åŒºåŸŸ ---
    # ä½ çš„ CPU æœ‰ 20 ä¸ªé€»è¾‘æ ¸å¿ƒã€‚
    # ç•™å‡ ä¸ªç»™ç³»ç»Ÿï¼Œå¼€ 12-16 ä¸ªæ˜¯æ¯”è¾ƒåˆç†çš„ã€‚
    # æ³¨æ„ï¼šå¼€å¤ªå¤šä¼šå¯¼è‡´å†…å­˜å‹åŠ›è¿‡å¤§ (æ¯ä¸ªè¿›ç¨‹éƒ½è¦åŠ è½½ä¸€ä¸ª CT)
    NUM_WORKERS = 12
    BATCH_SIZE = 1 # å¤„ç†æ˜¯ä»¥ CT ä¸ºå•ä½çš„ï¼ŒBatch Size è®¾ä¸º 1 å³å¯ï¼Œæ–¹ä¾¿çœ‹è¿›åº¦

    DATA_DIR = "./datasets/LUNA16"
    CSV_PATH = "./datasets/LUNA16/candidates_V2.csv"
    # ----------------

    print(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹ç¼“å­˜é¢„å¤„ç† (Workers: {NUM_WORKERS})...")

    # 1. åˆå§‹åŒ–åŸå§‹ Dataset (ä¸ºäº†è·å–è·¯å¾„å’Œé€»è¾‘)
    # val_stride=1 ç¡®ä¿æˆ‘ä»¬åœ¨å†…éƒ¨æ‹¿åˆ°æ‰€æœ‰çš„ UID
    base_ds = LunaDataset(
        data_dir=DATA_DIR,
        csv_path=CSV_PATH,
        is_val_set=True,
        val_stride=1
    )

    # 2. åŒ…è£…æˆæŒ‰ CT éå†çš„ Dataset
    pre_ds = PrecacheDataset(base_ds)

    # 3. ä½¿ç”¨ DataLoader å®ç°å¤šè¿›ç¨‹
    # collate_fn=None: å› ä¸ºæˆ‘ä»¬è¿”å›çš„æ˜¯ç®€å•çš„ True/Falseï¼Œä¸éœ€è¦å¤æ‚çš„æ‹¼æ¥
    loader = DataLoader(
        pre_ds,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        shuffle=False, # é¡ºåºå¤„ç†å³å¯
        collate_fn=lambda x: x # ç®€å•çš„å ä½ç¬¦ï¼Œä¸è¿›è¡Œ tensor è½¬æ¢
    )

    # 4. å¼€å§‹å¾ªç¯
    start_time = time.time()

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    # desc: è¿›åº¦æ¡å·¦è¾¹çš„æ–‡å­—
    # unit: å•ä½
    success_count = 0
    with tqdm(total=len(pre_ds), desc="Caching CTs", unit="scan") as pbar:
        for results in loader:
            # results æ˜¯ä¸€ä¸ª batch çš„ True/False åˆ—è¡¨
            success_count += sum(results)
            pbar.update(len(results))

    duration = (time.time() - start_time) / 60
    print(f"\nâœ… å®Œæˆï¼")
    print(f"è€—æ—¶: {duration:.2f} åˆ†é’Ÿ")
    print(f"æˆåŠŸå¤„ç†: {success_count}/{len(pre_ds)}")

if __name__ == '__main__':
    # Windows/Linux å¤šè¿›ç¨‹å¿…é¡»åœ¨ __main__ ä¿æŠ¤ä¸‹è¿è¡Œ
    main()
